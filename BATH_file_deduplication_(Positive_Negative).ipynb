{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NehaSontakk/BATH-Prokka-Comparison/blob/main/BATH_file_deduplication_(Positive_Negative).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnjyCqJT0du3"
      },
      "outputs": [],
      "source": [
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o40Du17k1Evb"
      },
      "outputs": [],
      "source": [
        "e_value_threshold = 0.000001"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QMPsqDZF1HBZ"
      },
      "outputs": [],
      "source": [
        "class filtering_operations:\n",
        "\n",
        "  @staticmethod\n",
        "  def e_value_filtering(df):\n",
        "    return df.loc[df['E-value']<=e_value_threshold]\n",
        "\n",
        "  @staticmethod\n",
        "  def pos_neg_strand_filtering(df_filtered):\n",
        "    #df should be post e_value_filteration\n",
        "    df = filtering_operations.e_value_filtering(df_filtered)\n",
        "    #Strand identification\n",
        "    df['ali from'] = df['ali from'].astype(int)\n",
        "    df['ali to'] = df['ali to'].astype(int)\n",
        "    strand = []\n",
        "\n",
        "    for index,row in df.iterrows():\n",
        "      if row['ali from'] < row['ali to']:\n",
        "        strand.append(\"+\")\n",
        "      elif row['ali from'] > row['ali to']:\n",
        "        strand.append(\"-\")\n",
        "\n",
        "    df['strand'] = strand\n",
        "    return df\n",
        "\n",
        "  @staticmethod\n",
        "  def get_specific_strand(df_raw,strand_info):\n",
        "    df = filtering_operations.pos_neg_strand_filtering(df_raw)\n",
        "    return df.loc[df['strand']==strand_info]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Pzc0nmh95ls2"
      },
      "outputs": [],
      "source": [
        "class overlap_deduplications:\n",
        "\n",
        "  ali_to_column = \"\"\n",
        "  ali_from_column = \"\"\n",
        "\n",
        "  #METHODS FOR 100% DEDUPLICATION\n",
        "\n",
        "  @staticmethod\n",
        "  def handle_group(group):\n",
        "    #Now for each element in group\n",
        "    min_e_value = group['E-value'].min()\n",
        "    #find elements with lowest e-values\n",
        "    group_min_e = group.loc[group['E-value'] == min_e_value]\n",
        "    #if group contains a single row save it\n",
        "    if len(group_min_e) == 1:\n",
        "      return group_min_e\n",
        "    #if group contains multiple rows with same e-values check min score\n",
        "    elif len(group_min_e) > 1:\n",
        "      print(\"Group contains multiple rows\")\n",
        "      return group_min_e.iloc[0:1]\n",
        "\n",
        "  @staticmethod\n",
        "  def deduplicate_full_overlaps(df):\n",
        "    #100% Overlap Dedup\n",
        "    print(\"Number of elements in relevant strand: \",df.shape)\n",
        "    #Find all the duplicates\n",
        "    duplicates = df.duplicated(subset=['target name',overlap_deduplications.ali_from_column, overlap_deduplications.ali_to_column], keep=False)\n",
        "    #make a duplicates df\n",
        "    duplicates_df = df[duplicates]\n",
        "    #make a non duplicates df\n",
        "    not_duplicate_df = df[~duplicates]\n",
        "    print(\"Number of exact duplicates or homologs: \",duplicates_df.shape)\n",
        "    #Sort by E-value and score\n",
        "    duplicates_df1 = duplicates_df.sort_values(by=['E-value', 'score'], ascending=[True, False])\n",
        "    deduplicated_remhomologs = duplicates_df1.groupby(['target name',overlap_deduplications.ali_from_column,overlap_deduplications.ali_to_column], group_keys=False).apply(overlap_deduplications.handle_group).reset_index(drop=True)\n",
        "    #rejion after deduplication\n",
        "    dedup_step1 = pd.concat([not_duplicate_df,deduplicated_remhomologs]).sort_values(['target name','ali from','E-value'])\n",
        "    print(\"Number of elements after removing exact duplicates or homologs: \",dedup_step1.shape)\n",
        "    print(\"Elements removed: \",df.shape[0] - dedup_step1.shape[0])\n",
        "    return dedup_step1\n",
        "\n",
        "  #METHODS FOR 70% DEDUPLICATION\n",
        "\n",
        "  @staticmethod\n",
        "  def calculate_overlap(hit_a, hit_b):\n",
        "    #print(\"Overlap calculations done on columns:\",overlap_deduplications.ali_from_column,overlap_deduplications.ali_to_column)\n",
        "    start_a, end_a = hit_a[overlap_deduplications.ali_from_column], hit_a[overlap_deduplications.ali_to_column]\n",
        "    start_b, end_b = hit_b[overlap_deduplications.ali_from_column], hit_b[overlap_deduplications.ali_to_column]\n",
        "\n",
        "    overlap_length = max(0, min(end_a, end_b) - max(start_a, start_b) + 1)\n",
        "    #print(min(end_a, end_b))\n",
        "    #print(max(start_a, start_b))\n",
        "    #print(\"Overlap length\",overlap_length)\n",
        "\n",
        "    length_a = end_a - start_a + 1\n",
        "    length_b = end_b - start_b + 1\n",
        "\n",
        "    #print(\"Length\",length_a,length_b)\n",
        "    if length_a > 0 and length_b > 0:\n",
        "        overlap_perc_a = (overlap_length / length_a) * 100\n",
        "        overlap_perc_b = (overlap_length / length_b) * 100\n",
        "        return overlap_perc_a, overlap_perc_b\n",
        "    else:\n",
        "        return 0, 0\n",
        "\n",
        "  @staticmethod\n",
        "  def calculate_winner(hit_a, hit_b):\n",
        "      overlap1, overlap2 = overlap_deduplications.calculate_overlap(hit_a, hit_b)\n",
        "      print(f\"Comparing:\\nHit A: {hit_a}\\nHit B: {hit_b}\\nOverlap1: {overlap1}%, Overlap2: {overlap2}%\")\n",
        "\n",
        "      # Check for significant overlap\n",
        "      if overlap1 >= 70 or overlap2 >= 70:\n",
        "          # Determine which hit to retain based on E-value or sequence length on tie\n",
        "          if hit_a['E-value'] < hit_b['E-value']:\n",
        "              print(\"Choosing Hit A based on E-value\")\n",
        "              return hit_a\n",
        "          elif hit_a['E-value'] > hit_b['E-value']:\n",
        "              print(\"Choosing Hit B based on E-value\")\n",
        "              return hit_b\n",
        "          else:  # E-values are tied, check the sequence length\n",
        "              if hit_a['seq len'] > hit_b['seq len']:\n",
        "                  print(\"E-values tied. Choosing Hit A based on larger sequence length\")\n",
        "                  return hit_a\n",
        "              else:\n",
        "                  print(\"E-values tied. Choosing Hit B based on larger sequence length\")\n",
        "                  return hit_b\n",
        "      else:\n",
        "          print(\"No significant overlap or both hits retained\")\n",
        "          return hit_a, hit_b\n",
        "\n",
        "  @staticmethod\n",
        "  def handle_70_overlaps(group):\n",
        "    # Check if the DataFrame has 1 or fewer rows\n",
        "    if len(group) <= 1:\n",
        "        return group\n",
        "\n",
        "\n",
        "    items = []\n",
        "    #print(\"HIIII\")\n",
        "    #print(group)\n",
        "\n",
        "    for index, row in group.iterrows():\n",
        "      row_tuple = row.to_dict()\n",
        "      items.append(row_tuple)\n",
        "\n",
        "    i = 0\n",
        "    all_winners = []\n",
        "\n",
        "    while i<len(items)-1:\n",
        "      current = items[i]\n",
        "      print(current)\n",
        "      next_item = items[i+1]\n",
        "      print(next_item)\n",
        "      print(\"Begining Comparison: \",i)\n",
        "      #print(current,\"\\n\",next_item)\n",
        "      result = overlap_deduplications.calculate_winner(current, next_item)\n",
        "\n",
        "      #If result is two items append the current item to winners list since overlap of current item with next item is less than 70% so we want to keep both\n",
        "      if isinstance(result,tuple):\n",
        "        if result[0] not in all_winners:\n",
        "          all_winners.append(result[0])\n",
        "          #print(\"Tuple returned!\")\n",
        "          #print(\"Appending: \",result[0])\n",
        "          #print(\"All winners currently: \",all_winners)\n",
        "        #else:\n",
        "        #  print(\"Result already in all winners, no appending!\")\n",
        "        i += 1\n",
        "      else:\n",
        "        if result == current:\n",
        "          #print(\"Result is \",i,\" element and not \",i+1,\" popping: \",items[i+1])\n",
        "          items.pop(i+1)\n",
        "        else:\n",
        "          i += 1\n",
        "\n",
        "      if i >= len(items) - 1 and items[-1] not in all_winners:\n",
        "        all_winners.append(items[-1])\n",
        "\n",
        "\n",
        "    return pd.DataFrame(all_winners)\n",
        "\n",
        "  #METHODS FOR LESS THAN 70% DEDUPLICATION\n",
        "  @staticmethod\n",
        "  def calculate_below_70_changes(hit_a, hit_b):\n",
        "    overlap1, overlap2 = overlap_deduplications.calculate_overlap(hit_a, hit_b)\n",
        "    #print(\"Overlap 1: \",overlap1)\n",
        "    if (0.01 <= overlap1 < 70) and (0.01 <= overlap2 < 70):\n",
        "      if hit_a['E-value'] < hit_b['E-value']:\n",
        "        print(\"Case A: A dominates\")\n",
        "        print(hit_a)\n",
        "        print(hit_b)\n",
        "        hit_b['ali from'] = hit_a['ali to'] + 1\n",
        "        #print(hit_b)\n",
        "        return hit_a, hit_b\n",
        "      else:\n",
        "        print(\"Case B: B dominates\")\n",
        "        print(hit_a)\n",
        "        print(hit_b)\n",
        "        hit_a['ali to'] = hit_b['ali from'] - 1\n",
        "        #print(hit_a)\n",
        "        #print(hit_b)\n",
        "        return hit_a, hit_b\n",
        "    else:\n",
        "      return hit_a,hit_b\n",
        "\n",
        "  @staticmethod\n",
        "  def below_70_overlap(group):\n",
        "\n",
        "    if len(group) <= 1:\n",
        "        print(\"Length of group less than 1.\")\n",
        "        return group\n",
        "\n",
        "    items = []\n",
        "    for index, row in group.iterrows():\n",
        "      row_tuple = row.to_dict()\n",
        "      items.append(row_tuple)\n",
        "\n",
        "    for i in range(0,len(items)-1):\n",
        "      element1, element2 = overlap_deduplications.calculate_below_70_changes(items[i],items[i+1])\n",
        "      items[i] = element1\n",
        "      items[i+1] = element2\n",
        "\n",
        "    return pd.DataFrame(items)\n",
        "\n",
        "  # FINAL METHOD TO CALL\n",
        "  @staticmethod\n",
        "  def choose_strand_operations(df):\n",
        "    if df['strand'].values[0] == '-':\n",
        "      print(\"Processing negative strand data...\")\n",
        "      #create the flip columns\n",
        "      df['ali from flip'] = df['ali to']\n",
        "      df['ali to flip'] = df['ali from']\n",
        "      overlap_deduplications.ali_to_column = 'ali to flip'\n",
        "      overlap_deduplications.ali_from_column = 'ali from flip'\n",
        "      #100% dedup\n",
        "      df = df.sort_values(by=['ali from flip','E-value'], ascending=[True, True])\n",
        "      print(df)\n",
        "      step1_dedup = overlap_deduplications.deduplicate_full_overlaps(df)\n",
        "      print(\"Overlap calculations done on columns:\",overlap_deduplications.ali_from_column,overlap_deduplications.ali_to_column)\n",
        "      print(\"100% deduplication done.\")\n",
        "      #70% Overlap\n",
        "      step2_dedup = step1_dedup.groupby(['target name'], group_keys=False).apply(overlap_deduplications.handle_70_overlaps).reset_index(drop=True)\n",
        "      print(\"Overlap calculations done on columns:\",overlap_deduplications.ali_from_column,overlap_deduplications.ali_to_column)\n",
        "      print(\"70% deduplication done.\")\n",
        "      #Less than 70% overlap\n",
        "      step3_dedup = step2_dedup.groupby(['target name'], group_keys=False).apply(overlap_deduplications.below_70_overlap).reset_index(drop=True)\n",
        "      print(\"Less than 70% deduplication done.\")\n",
        "      #return something final\n",
        "      return step3_dedup\n",
        "    elif df['strand'].values[0] == '+':\n",
        "      print(\"Processing positive strand data...\")\n",
        "      overlap_deduplications.ali_to_column = 'ali to'\n",
        "      overlap_deduplications.ali_from_column = 'ali from'\n",
        "      #100% dedup\n",
        "      df = df.sort_values(by=['ali from','E-value'], ascending=[True, True])\n",
        "      print(df)\n",
        "      step1_dedup = overlap_deduplications.deduplicate_full_overlaps(df)\n",
        "      print(\"Overlap calculations done on columns:\",overlap_deduplications.ali_from_column,overlap_deduplications.ali_to_column)\n",
        "      print(\"100% deduplication done.\")\n",
        "      #70% Overlap\n",
        "      step2_dedup = step1_dedup.groupby(['target name'], group_keys=False).apply(overlap_deduplications.handle_70_overlaps).reset_index(drop=True)\n",
        "      print(\"Overlap calculations done on columns:\",overlap_deduplications.ali_from_column,overlap_deduplications.ali_to_column)\n",
        "      print(\"70% deduplication done.\")\n",
        "      #Less than 70% overlap\n",
        "      print(step2_dedup)\n",
        "      step3_dedup = step2_dedup.groupby(['target name'], group_keys=False).apply(overlap_deduplications.below_70_overlap).reset_index(drop=True)\n",
        "      print(\"Less than 70% deduplication done.\")\n",
        "      #return something final\n",
        "      print(step3_dedup)\n",
        "      return step3_dedup\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Trying out something New\n",
        "from glob import glob\n",
        "dataframes = []\n",
        "col_names = [\n",
        "    'target name', 'accession', 'query name', 'accession1', 'hmm len',\n",
        "    'hmm from', 'hmm to', 'seq len', 'ali from', 'ali to',\n",
        "    'env from', 'env to', 'E-value', 'score', 'bias', 'shifts',\n",
        "    'stops', 'pipe', 'description of target', 'extra'\n",
        "]\n",
        "for file_path in glob(\"/content/drive/MyDrive/Lab Work/Parkinsons_Data/Deduplication Testing 31Jul2024/bathoutprededup/*.tbl\"):\n",
        "    # Read the table\n",
        "    bathout = pd.read_table(file_path, sep=\"\\s+\", header=None, skiprows=2, skipfooter=8, engine='python')\n",
        "\n",
        "    # Rename the columns\n",
        "    bathout.rename(columns=dict(zip(bathout.columns, col_names)), inplace=True)\n",
        "\n",
        "    # Append the DataFrame to the list\n",
        "    dataframes.append(bathout)\n",
        "\n",
        "# Concatenate all DataFrames into a single DataFrame\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)"
      ],
      "metadata": {
        "id": "BXJHbIXIEA7p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_neg = filtering_operations.get_specific_strand(combined_df,\"-\")\n",
        "neg_deduped = overlap_deduplications.choose_strand_operations(df_neg)\n",
        "df_pos = filtering_operations.get_specific_strand(combined_df,\"+\")\n",
        "pos_deduped = overlap_deduplications.choose_strand_operations(df_pos)\n",
        "complete_bath_deduplication = pd.concat([pos_deduped,neg_deduped])"
      ],
      "metadata": {
        "collapsed": true,
        "id": "8gczGG9rFPEf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "complete_bath_deduplication.shape"
      ],
      "metadata": {
        "id": "ew7SrKkoFvAW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gH3k_ICnUZVn"
      },
      "outputs": [],
      "source": [
        "complete_bath_deduplication.to_excel(\"AUG1bin82_BATH_deduplicated.xlsx\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0qtFhQW1PNJS"
      },
      "outputs": [],
      "source": [
        "'''\n",
        "#Do it for each individual bath file first\n",
        "from glob import glob\n",
        "#SPECIFY LOCATION\n",
        "for i in glob(\"/content/drive/MyDrive/Lab Work/Parkinsons_Data/Deduplication Testing 31Jul2024/bathoutprededup/*.tbl\"):\n",
        "  bathout = pd.read_table(\"/content/drive/MyDrive/Lab Work/Parkinsons_Data/Bath_Output/DNA_Bacteria_kingdom_sprot.tbl\",sep=\"\\s+\",header=None,skiprows=2,skipfooter=8)\n",
        "  col_names = ['target name', 'accession', 'query name', 'accession1', 'hmm len', 'hmm from', 'hmm to', 'seq len', 'ali from', 'ali to', 'env from', 'env to', 'E-value', 'score', 'bias', 'shifts', 'stops', 'pipe', 'description of target', 'extra']\n",
        "  bathout.rename(columns=dict(zip(bathout.columns, col_names)), inplace=True)\n",
        "  # Filter the DataFrame for negative strand entries\n",
        "  df_neg = filtering_operations.get_specific_strand(bathout,\"-\")\n",
        "  neg_deduped = overlap_deduplications.choose_strand_operations(df_neg)\n",
        "  # Filter the DataFrame for positive strand entries\n",
        "  df_pos = filtering_operations.get_specific_strand(bathout,\"+\")\n",
        "  pos_deduped = overlap_deduplications.choose_strand_operations(df_pos)\n",
        "  bath_file_deduplication = pd.concat([pos_deduped,neg_deduped])\n",
        "  # Add a new column indicating the origin of the file\n",
        "  bath_file_deduplication['origin'] = str(i.split(\"/\")[-1].split(\".tbl\")[0])\n",
        "  # Save the deduplicated DataFrame to an Excel file\n",
        "  bath_file_deduplication.to_excel(str(i.split(\"/\")[-1].split(\".tbl\")[0]+\".xlsx\"),index=False)\n",
        "\n",
        "#Combine the bathfiles\n",
        "list_of_dfs = []\n",
        "for i in glob(\"/content/*.xlsx\"):\n",
        "  list_of_dfs.append(pd.read_excel(i))\n",
        "\n",
        "#Repeat for the combined bathfile\n",
        "bathout = pd.concat(list_of_dfs)\n",
        "df_neg = filtering_operations.get_specific_strand(bathout,\"-\")\n",
        "neg_deduped = overlap_deduplications.choose_strand_operations(df_neg)\n",
        "df_pos = filtering_operations.get_specific_strand(bathout,\"+\")\n",
        "pos_deduped = overlap_deduplications.choose_strand_operations(df_pos)\n",
        "complete_bath_deduplication = pd.concat([pos_deduped,neg_deduped])\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Single File Test"
      ],
      "metadata": {
        "id": "zuyFeb2zDFCS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bathout = pd.read_table('/content/test_case1.tbl', sep=\"\\s+\",skiprows =1, header=None)\n",
        "bathout\n",
        "# Define column names\n",
        "col_names = ['target name', 'accession', 'query name', 'accession1', 'hmm len', 'hmm from', 'hmm to',\n",
        "              'seq len', 'ali from', 'ali to', 'env from', 'env to', 'E-value', 'score', 'bias',\n",
        "              'shifts', 'stops', 'pipe', 'description of target', 'extra']\n",
        "\n",
        "# Rename columns\n",
        "bathout.rename(columns=dict(zip(bathout.columns, col_names)), inplace=True)\n",
        "\n",
        "# Process negative strand\n",
        "#df_neg = filtering_operations.get_specific_strand(bathout, \"-\")\n",
        "#neg_deduped = overlap_deduplications.choose_strand_operations(df_neg)\n",
        "\n",
        "# Process positive strand\n",
        "df_pos = filtering_operations.get_specific_strand(bathout, \"+\")\n",
        "pos_deduped = overlap_deduplications.choose_strand_operations(df_pos)\n"
      ],
      "metadata": {
        "id": "9u7DjIlqxbgL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos_deduped"
      ],
      "metadata": {
        "id": "DVwUMceD3zlb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "YFrKkqadsrd0"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "https://github.com/NehaSontakk/BATH-Prokka-Comparison/blob/main/BATH_file_deduplication_(Positive_Negative).ipynb",
      "authorship_tag": "ABX9TyMhA3SUqC+Bpz/Z8HUMxZAr",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}