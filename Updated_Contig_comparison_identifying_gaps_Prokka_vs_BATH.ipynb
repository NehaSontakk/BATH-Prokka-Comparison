{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NehaSontakk/BATH-Prokka-Comparison/blob/main/Updated_Contig_comparison_identifying_gaps_Prokka_vs_BATH.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Alignment and Classification of BATH and Prokka Annotations\n",
        "\n",
        "Performs a comprehensive comparison between genomic annotations produced by Prokka and BATH. First the BED files from Prokka and BATH annotation outputs are prepared, identifying overlaps and unique regions, and visualizing the results.\n",
        "\n",
        "The code generates BED files for different genomic regions, segregates them based on DNA strands, and runs operations to find overlaps and unique annotations between the two datasets. It combines the resulting files by prefix, ensuring that all directories are created if they don't exist. Finally, the code analyzes the overlapping and unique regions, categorizes them, and visualizes the data with a Venn diagram to provide insights into the annotations."
      ],
      "metadata": {
        "id": "7QuRDo5li6wB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BqNYSv6KEJar"
      },
      "outputs": [],
      "source": [
        "#Installations\n",
        "!pip install gffpandas\n",
        "!sudo apt-get install bedops\n",
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#File paths\n",
        "prokka_annotations = '/content/drive/MyDrive/Lab Work/Parkinsons_Data/BIN152/Prokka Output/bin152.gff'\n",
        "bath_dedup_annotations = \"/content/drive/MyDrive/Lab Work/Parkinsons_Data/BIN152/Deduplication Output/dedup_bin52_test1.xlsx\""
      ],
      "metadata": {
        "id": "4olqJ_VdjczD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import os\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "import gffpandas.gffpandas as gffpd\n",
        "from Bio import SeqIO\n",
        "from glob import glob\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "flkVBtNfm0za"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and prep the prokka files\n",
        "annotation = gffpd.read_gff3(prokka_annotations)\n",
        "combined_df = annotation.filter_feature_of_type(['CDS'])\n",
        "attr_to_columns = combined_df.attributes_to_columns()\n",
        "prokka_proteins = pd.DataFrame(attr_to_columns)\n",
        "prokka_proteins['start'] = prokka_proteins['start'].astype(int)\n",
        "prokka_proteins['end'] = prokka_proteins['end'].astype(int)"
      ],
      "metadata": {
        "id": "p5kpIpZYxapX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prokka_proteins.head()"
      ],
      "metadata": {
        "id": "6SYJN5-hz5Bd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and prep bath files\n",
        "bath_protein = pd.read_excel(bath_dedup_annotations)\n",
        "unnamed_cols = [col for col in bath_protein.columns if col.startswith('Unnamed')]\n",
        "bath_protein.drop(columns=unnamed_cols, inplace=True)\n",
        "bath_protein['start'] = bath_protein['ali from']\n",
        "bath_protein['end'] = bath_protein['ali to']\n",
        "bath_protein['start'].fillna(bath_protein['ali from flip'],inplace=True)\n",
        "bath_protein['end'].fillna(bath_protein['ali to flip'],inplace=True)"
      ],
      "metadata": {
        "id": "tNkhZx66xhIe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bath_protein.head()"
      ],
      "metadata": {
        "id": "u83MBX-cz9Wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find the gaps filled by BATH vs Prokka per contig"
      ],
      "metadata": {
        "id": "x2Um3f-1fJa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add 'ID' to bath_protein\n",
        "bath_protein['ID'] = bath_protein['query name']\n",
        "\n",
        "# Separate BATH data into positive and negative strands\n",
        "pos_strand = bath_protein[bath_protein['strand'] == \"+\"].copy()\n",
        "neg_strand = bath_protein[bath_protein['strand'] == \"-\"].copy()\n",
        "\n",
        "# Handle columns for positive strand\n",
        "pos_strand.loc[:, 'start'] = pos_strand['ali from'].astype(int)\n",
        "pos_strand.loc[:, 'end'] = pos_strand['ali to'].astype(int)\n",
        "\n",
        "# Handle columns for negative strand\n",
        "neg_strand.loc[:, 'start'] = neg_strand['ali from flip'].astype(int)\n",
        "neg_strand.loc[:, 'end'] = neg_strand['ali to flip'].astype(int)\n",
        "\n",
        "# Filter Prokka proteins into positive and negative strands\n",
        "prokka_proteins_plus = prokka_proteins[prokka_proteins['strand'] == '+'].copy()\n",
        "prokka_proteins_minus = prokka_proteins[prokka_proteins['strand'] == '-'].copy()\n",
        "\n",
        "# Sort and save positive strand Prokka proteins\n",
        "prokka_proteins_plus.sort_values('start', inplace=True)\n",
        "prokka_proteins_plus.to_csv('prokka_proteins_plus.bed', sep='\\t', index=False, header=False, columns=['seq_id', 'start', 'end', 'strand', 'ID', 'inference'])\n",
        "\n",
        "# Sort and save negative strand Prokka proteins\n",
        "prokka_proteins_minus.sort_values('start', inplace=True)\n",
        "prokka_proteins_minus.to_csv('prokka_proteins_minus.bed', sep='\\t', index=False, header=False, columns=['seq_id', 'start', 'end', 'strand', 'ID', 'inference'])\n",
        "\n",
        "# Sort and save BATH deduplicated positive strand\n",
        "bath_deduplicated_plus = pos_strand[['target name', 'start', 'end', 'strand', 'ID', 'shifts']].copy()\n",
        "bath_deduplicated_plus.sort_values('start', inplace=True)\n",
        "bath_deduplicated_plus.to_csv('bath_deduplicated_plus.bed', sep='\\t', index=False, header=False)\n",
        "\n",
        "# Sort and save BATH deduplicated negative strand\n",
        "bath_deduplicated_minus = neg_strand[['target name', 'start', 'end', 'strand', 'ID', 'shifts']].copy()\n",
        "bath_deduplicated_minus.sort_values('start', inplace=True)\n",
        "bath_deduplicated_minus.to_csv('bath_deduplicated_minus.bed', sep='\\t', index=False, header=False)\n"
      ],
      "metadata": {
        "id": "WgQ2JY2zc1KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions\n",
        "\n",
        "\n",
        "\n",
        "1.   generate_bed_files:\n",
        "This function creates BED files by iterating through unique contig identifiers in the DataFrame, filtering and sorting data by start positions, and writing the data to BED files.\n",
        "\n",
        "2.   run_bedmap_operations:\n",
        "This function uses bedmap and bedops tools to find overlapping and unique regions between BATH and Prokka BED files. It generates three output files: one for overlaps, one for regions unique to BATH, and one for regions unique to Prokka, saving them in the specified output directory\n",
        "\n",
        "3.   find_and_process_bed_pairs:\n",
        "This function finds and pairs BED files from BATH and Prokka directories based on sequence identifiers. It then calls run_bedmap_operations to process each pair, identifying overlaps and unique regions, and saves the results in the output directory.\n",
        "\n",
        "4.   combine_files_by_prefix:\n",
        "This function organizes and combines BED files in a source directory by their prefixes. It groups files with the same prefix, concatenates their contents, and writes the combined data into new BED files in the output directory. Each new file is named with the prefix followed by \"combined.bed\"."
      ],
      "metadata": {
        "id": "KDjfnh2fuHEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Functions\n",
        "\n",
        "#generate BED files from a dataframe for each unique sequence identifier (contig) present in the dataframe.\n",
        "def generate_bed_files(df, directory, contig_column, file_prefix='Prokka_annotation'):\n",
        "    # Generates BED files for each unique 'seq_id' in the DataFrame.\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "    for seq_id in df[contig_column].unique():\n",
        "        df_seq = df[df[contig_column] == seq_id]\n",
        "        #print(df_seq.shape)\n",
        "        df_seq = df[df[contig_column] == seq_id].sort_values(by=['start'])\n",
        "        filename = f'{directory}/{file_prefix}_{seq_id}.bed'\n",
        "        with open(filename, 'w') as file:\n",
        "            for _, row in df_seq.iterrows():\n",
        "              #For a prokka file\n",
        "              if file_prefix == \"Prokka_annotation\":\n",
        "                bed_line = f\"{row[contig_column]}\\t{row['start']}\\t{row['end']}\\t{row['ID']}\\t{row['strand']}\\t{row['inference']}\\n\"\n",
        "                file.write(bed_line)\n",
        "              else:\n",
        "                #For a BATH file\n",
        "                bed_line = f\"{row[contig_column]}\\t{row['start']}\\t{row['end']}\\t{row['ID']}\\t{row['strand']}\\t{row['shifts']}\\n\"\n",
        "                file.write(bed_line)\n",
        "\n",
        "    print(f\"BED files have been successfully generated in {directory}\")\n",
        "\n",
        "def run_bedmap_operations(bath_bed_path, prokka_bed_path, output_base_dir):\n",
        "    seq_id = bath_bed_path.stem.split('_')[-1]\n",
        "    overlap_output = output_base_dir / f'overlap_prokka_bath_{seq_id}.bed'\n",
        "    unique_to_bath_output = output_base_dir / f'unique_to_bath_{seq_id}.bed'\n",
        "    unique_to_prokka_output = output_base_dir / f'unique_to_prokka_{seq_id}.bed'\n",
        "\n",
        "    output_base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Command for bedmap to find overlaps\n",
        "    command = [\n",
        "        \"bedmap\", \"--echo\", \"--echo-map\", \"--delim\", \";\",\n",
        "        \"--fraction-ref\", \"0.15\",\n",
        "        str(bath_bed_path), str(prokka_bed_path)\n",
        "    ]\n",
        "\n",
        "    # Run the command and capture output\n",
        "    result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
        "\n",
        "    # Filter the output and write to overlap_output\n",
        "    with open(overlap_output, 'w') as f_output:\n",
        "        for line in result.stdout.splitlines():\n",
        "            parts = line.split(';')\n",
        "            # Check if there is more than one part and the second part is not empty\n",
        "            if len(parts) > 1 and parts[1].strip():\n",
        "                f_output.write(line + '\\n')\n",
        "\n",
        "    # Run bedmap for unique to BATH\n",
        "    subprocess.run([\"bedops\", \"--not-element-of\", \"1\", str(bath_bed_path), str(prokka_bed_path)], stdout=open(unique_to_bath_output, 'w'), check=True)\n",
        "\n",
        "    # Run bedmap for unique to Prokka\n",
        "    subprocess.run([\"bedops\", \"--not-element-of\", \"1\", str(prokka_bed_path), str(bath_bed_path)], stdout=open(unique_to_prokka_output, 'w'), check=True)\n",
        "\n",
        "def find_and_process_bed_pairs(bath_dir, prokka_dir, output_dir):\n",
        "    bath_dir_path = Path(bath_dir)\n",
        "    prokka_dir_path = Path(prokka_dir)\n",
        "    output_base_dir = Path(output_dir)\n",
        "\n",
        "    bath_bed_files = {f.stem.split('_')[-1]: f for f in bath_dir_path.glob('BATH_annotation_*.bed')}\n",
        "    prokka_bed_files = {f.stem.split('_')[-1]: f for f in prokka_dir_path.glob('Prokka_annotation_*.bed')}\n",
        "\n",
        "    for seq_id, bath_bed_path in bath_bed_files.items():\n",
        "        print(seq_id)\n",
        "        prokka_bed_path = prokka_bed_files.get(seq_id)\n",
        "        print(\"Processing files ... \",prokka_bed_path, \"and \",bath_bed_path)\n",
        "        if prokka_bed_path:\n",
        "            run_bedmap_operations(bath_bed_path, prokka_bed_path, output_base_dir)\n",
        "        else:\n",
        "            print(f\"No matching Prokka file found for BATH seq_id: {seq_id}\")\n",
        "\n",
        "def combine_files_by_prefix(source_directory, output_directory, prefixes):\n",
        "    source_directory = Path(source_directory)\n",
        "    output_directory = Path(output_directory)\n",
        "    output_directory.mkdir(parents=True, exist_ok=True)\n",
        "    file_groups = {prefix: [] for prefix in prefixes}\n",
        "    for file_path in source_directory.glob('*.bed'):\n",
        "        for prefix in prefixes:\n",
        "            if file_path.name.startswith(prefix):\n",
        "                file_groups[prefix].append(file_path)\n",
        "    for prefix, files in file_groups.items():\n",
        "        combined_file_path = output_directory / f\"{prefix}combined.bed\"\n",
        "        with combined_file_path.open('w') as combined_file:\n",
        "            for file_path in files:\n",
        "                with file_path.open() as input_file:\n",
        "                    combined_file.write(input_file.read())\n",
        "                combined_file.write('\\n')"
      ],
      "metadata": {
        "id": "sKpfkWmmqSr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "BASE_DIR = '/content/BED'\n",
        "PROKKA_PLUS_DIR = f'{BASE_DIR}/Prokka_Annotation_PLUS_BED'\n",
        "PROKKA_MINUS_DIR = f'{BASE_DIR}/Prokka_Annotation_MINUS_BED'\n",
        "BATH_PLUS_DIR = f'{BASE_DIR}/BATH_Annotation_PLUS_BED'\n",
        "BATH_MINUS_DIR = f'{BASE_DIR}/BATH_Annotation_MINUS_BED'\n",
        "OVERLAPS_DIR = f'{BASE_DIR}/Overlaps and Differences'\n",
        "COMBINED_FILES_DIR = f'{BASE_DIR}/Combined Files'\n",
        "\n",
        "\n",
        "def ensure_directories():\n",
        "    directories = [\n",
        "        PROKKA_PLUS_DIR, PROKKA_MINUS_DIR,\n",
        "        BATH_PLUS_DIR, BATH_MINUS_DIR,\n",
        "        OVERLAPS_DIR, COMBINED_FILES_DIR\n",
        "    ]\n",
        "    for directory in directories:\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "\n",
        "\n",
        "def generate_all_bed_files():\n",
        "    generate_bed_files(prokka_proteins_plus, PROKKA_PLUS_DIR, 'seq_id', file_prefix='Prokka_annotation')\n",
        "    generate_bed_files(prokka_proteins_minus, PROKKA_MINUS_DIR, 'seq_id', file_prefix='Prokka_annotation')\n",
        "    generate_bed_files(bath_deduplicated_plus, BATH_PLUS_DIR, 'target name', file_prefix='BATH_annotation')\n",
        "    generate_bed_files(bath_deduplicated_minus, BATH_MINUS_DIR, 'target name', file_prefix='BATH_annotation')\n",
        "\n",
        "# Process BED pairs\n",
        "def process_bed_pairs():\n",
        "    find_and_process_bed_pairs(BATH_PLUS_DIR, PROKKA_PLUS_DIR, OVERLAPS_DIR)\n",
        "\n",
        "def combine_bed_files():\n",
        "    prefixes = ['unique_to_prokka_', 'unique_to_bath_', 'overlap_prokka_bath_']\n",
        "    combine_files_by_prefix(OVERLAPS_DIR, COMBINED_FILES_DIR, prefixes)\n",
        "\n",
        "ensure_directories()\n",
        "generate_all_bed_files()\n",
        "process_bed_pairs()\n",
        "combine_bed_files()\n"
      ],
      "metadata": {
        "id": "s_FPODJwyzm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis"
      ],
      "metadata": {
        "id": "j7Zptah4RNCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Venn diagram of region based overlaps"
      ],
      "metadata": {
        "id": "gH6-YAkM86Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_file_to_dataframe(file_path):\n",
        "    transformed_rows = []\n",
        "\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "        for line in lines:\n",
        "            fields = line.strip().split(';')\n",
        "            if len(fields) == 3:\n",
        "                new_row1 = [fields[0], fields[1]]\n",
        "                new_row2 = [fields[0], fields[2]]\n",
        "                transformed_rows.append(new_row1)\n",
        "                transformed_rows.append(new_row2)\n",
        "            else:\n",
        "                transformed_rows.append(fields)\n",
        "    df = pd.DataFrame(transformed_rows, columns=['BATH', 'Prokka']).replace(\"\\n\",np.nan).dropna()\n",
        "\n",
        "    return df"
      ],
      "metadata": {
        "id": "kYKjxCfh_erj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/BED/Combined Files/overlap_prokka_bath_combined.bed'\n",
        "overlaps = transform_file_to_dataframe(file_path)"
      ],
      "metadata": {
        "id": "Sm3hcogl89ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_label(row):\n",
        "    if row['Prokka'].endswith('ab initio prediction:Prodigal:002006'):\n",
        "        return \"BATH and Prokka Unannotated\"\n",
        "    else:\n",
        "        return \"BATH and Prokka Annotated\"\n",
        "\n",
        "overlaps['label'] = overlaps.apply(determine_label, axis=1)"
      ],
      "metadata": {
        "id": "hZ-8rJkR_9yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/BED/Combined Files/unique_to_bath_combined.bed'\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = [line.strip() for line in file.readlines() if line.strip()]\n",
        "unique_to_bath = pd.DataFrame(lines, columns=['BATH']).replace(\"\\n\",np.nan).dropna()\n",
        "unique_to_bath['Prokka'] = np.nan\n",
        "unique_to_bath['label'] = 'BATH'"
      ],
      "metadata": {
        "id": "FghlpHe7_tMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path3 = '/content/BED/Combined Files/unique_to_prokka_combined.bed'\n",
        "with open(file_path3, 'r') as file:\n",
        "    lines = [line.strip() for line in file.readlines() if line.strip()]\n",
        "unique_to_prokka = pd.DataFrame(lines, columns=['Prokka']).replace(\"\\n\",np.nan).dropna()\n",
        "unique_to_prokka['BATH'] = np.nan\n",
        "\n",
        "def determine_label2(row):\n",
        "    if row['Prokka'].endswith('ab initio prediction:Prodigal:002006'):\n",
        "        return \"Prokka Unannotated\"\n",
        "    else:\n",
        "        return \"Prokka Annotated\"\n",
        "\n",
        "unique_to_prokka['label'] = unique_to_prokka.apply(determine_label2, axis=1)"
      ],
      "metadata": {
        "id": "izo5DWE4E-Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "venn_data = pd.concat([unique_to_prokka,unique_to_bath,overlaps])"
      ],
      "metadata": {
        "id": "AsZvSBJ5_tPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "venn_data.to_excel(\"/content/drive/MyDrive/Lab Work/Parkinsons_Data/BIN152/Aligned Outputs/Aligned_Outputs_bin152.xlsx\",index=False)"
      ],
      "metadata": {
        "id": "F78xwpca3oYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "venn_data['label'].value_counts()"
      ],
      "metadata": {
        "id": "6AqVGwhPIexG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "venn_data[venn_data['label'] == \"Prokka Annotated\"]"
      ],
      "metadata": {
        "id": "EsIzLrkeIFk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib_venn import venn3\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Set the default style for clearer and more vibrant visuals\n",
        "plt.style.use('seaborn')\n",
        "venn_counts = {\n",
        "    '100': venn_data[venn_data['label'] == \"BATH\"].shape[0],  # Only BATH\n",
        "    '010': venn_data[venn_data['label'] == \"Prokka Annotated\"].shape[0],  # Only Prokka Annotated\n",
        "    '001': venn_data[venn_data['label'] == \"Prokka Unannotated\"].shape[0],  # Only Prokka Unannotated\n",
        "    '110': venn_data[venn_data['label'] == \"BATH and Prokka Annotated\"].shape[0],  # Intersection BATH and Prokka Annotated\n",
        "    '101': venn_data[venn_data['label'] == \"BATH and Prokka Unannotated\"].shape[0],  # Intersection BATH and Prokka Unannotated\n",
        "    '011': 0,  # Intersection of Prokka Annotated and Prokka Unannotated (not given)\n",
        "    '111': 0   # Intersection of all three (not applicable)\n",
        "}\n",
        "\n",
        "# Custom colors for the sets\n",
        "set_colors = (\"#de3028\", \"#1B263B\", \"#FFD700\")\n",
        "\n",
        "# Creating the Venn diagram with custom colors\n",
        "venn3(subsets=venn_counts, set_labels=('BATH Annotations', 'Prokka Annotations', 'ORFs (Prodigal)'), set_colors=set_colors)\n",
        "\n",
        "\n",
        "plt.title(\"Overlaping annotations BATH and Prokka based DNA position\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lufM0t3bIFod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You need to find the areas annotated by BATH in regions that only prodigal annotates\n",
        "#You also need to find BATH individual areas"
      ],
      "metadata": {
        "id": "WNh9vxN5oR5v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#You also need to quantify it somehow"
      ],
      "metadata": {
        "id": "f1fFNTc5uIJQ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}