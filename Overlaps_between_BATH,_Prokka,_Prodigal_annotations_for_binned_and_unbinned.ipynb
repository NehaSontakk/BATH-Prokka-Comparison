{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1L9WXzuc_jCLU2GUHjk8HV-ur8kpbqWcx",
      "authorship_tag": "ABX9TyPv3BVeVzipd64pl4B720/f",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NehaSontakk/BATH-Prokka-Comparison/blob/main/Overlaps_between_BATH%2C_Prokka%2C_Prodigal_annotations_for_binned_and_unbinned.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hSJOk7_rFnec",
        "outputId": "f26b01c4-1006-40ff-dcbd-7596ae4fa6f2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gffpandas\n",
            "  Downloading gffpandas-1.2.0.tar.gz (178 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/178.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m178.8/178.8 kB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: gffpandas\n",
            "  Building wheel for gffpandas (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gffpandas: filename=gffpandas-1.2.0-py2.py3-none-any.whl size=6247 sha256=0ad0772048f07afa9f16343922968dbac26320e2f09497ed18315ec563257213\n",
            "  Stored in directory: /root/.cache/pip/wheels/57/87/f1/1d0c74fbc5151562ba7953dc110a7d8c63c6c3229d025bc8cd\n",
            "Successfully built gffpandas\n",
            "Installing collected packages: gffpandas\n",
            "Successfully installed gffpandas-1.2.0\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libjansson4 tcsh\n",
            "The following NEW packages will be installed:\n",
            "  bedops libjansson4 tcsh\n",
            "0 upgraded, 3 newly installed, 0 to remove and 49 not upgraded.\n",
            "Need to get 1,922 kB of archives.\n",
            "After this operation, 10.2 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libjansson4 amd64 2.13.1-1.1build3 [32.4 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/universe amd64 tcsh amd64 6.21.00-1.1 [422 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/universe amd64 bedops amd64 2.4.40+dfsg-1 [1,467 kB]\n",
            "Fetched 1,922 kB in 0s (7,994 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 3.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package libjansson4:amd64.\n",
            "(Reading database ... 123622 files and directories currently installed.)\n",
            "Preparing to unpack .../libjansson4_2.13.1-1.1build3_amd64.deb ...\n",
            "Unpacking libjansson4:amd64 (2.13.1-1.1build3) ...\n",
            "Selecting previously unselected package tcsh.\n",
            "Preparing to unpack .../tcsh_6.21.00-1.1_amd64.deb ...\n",
            "Unpacking tcsh (6.21.00-1.1) ...\n",
            "Selecting previously unselected package bedops.\n",
            "Preparing to unpack .../bedops_2.4.40+dfsg-1_amd64.deb ...\n",
            "Unpacking bedops (2.4.40+dfsg-1) ...\n",
            "Setting up libjansson4:amd64 (2.13.1-1.1build3) ...\n",
            "Setting up tcsh (6.21.00-1.1) ...\n",
            "update-alternatives: using /bin/tcsh to provide /bin/csh (csh) in auto mode\n",
            "Setting up bedops (2.4.40+dfsg-1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "Collecting biopython\n",
            "  Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from biopython) (1.26.4)\n",
            "Downloading biopython-1.84-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m66.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: biopython\n",
            "Successfully installed biopython-1.84\n"
          ]
        }
      ],
      "source": [
        "!pip install gffpandas\n",
        "!sudo apt-get install bedops\n",
        "!pip install biopython"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import os\n",
        "import subprocess\n",
        "import matplotlib.pyplot as plt\n",
        "import gffpandas.gffpandas as gffpd\n",
        "from Bio import SeqIO\n",
        "from glob import glob\n",
        "from pathlib import Path"
      ],
      "metadata": {
        "id": "TaLQ0Mm2HzfA"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Input file paths\n",
        "prokka_annotations = '/content/drive/MyDrive/Lab Work/Parkinsons_Data/Iteration_Sep30/prokka_binned_unbinned_Oct7.xlsx'\n",
        "bath_dedup_annotations = '/content/drive/MyDrive/Lab Work/Parkinsons_Data/Iteration_Sep30/dedup_combinned_binned_unbinned_sep30.xlsx'\n",
        "list_of_bathsearch_outputs = glob(\"/content/drive/MyDrive/Lab Work/Parkinsons_Data/Iteration_Sep30/BATH_Deduplication/BATHOUTPUT/*\")\n",
        "#Output files\n",
        "prokka_annotations_save = '/content/drive/MyDrive/Lab Work/Parkinsons_Data/Iteration_Sep30/Prokka Output/Prokka_proteins.xlsx'\n",
        "bath_dedup_annotations_save = \"/content/drive/MyDrive/Lab Work/Parkinsons_Data/Iteration_Sep30/Deduplication Output/BATH_proteins.xlsx\"\n",
        "bath_prokka_alignment = \"/content/drive/MyDrive/Lab Work/Parkinsons_Data/Iteration_Sep30/Aligned Outputs/Aligned_Outputs.xlsx\"\n",
        "bathsearch_output_file = \"bathsearch_combined_file.csv\""
      ],
      "metadata": {
        "id": "JPK5weeLH4q7"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prokka_proteins = pd.read_excel(prokka_annotations)\n",
        "print(prokka_proteins.shape)\n",
        "prokka_proteins = prokka_proteins[prokka_proteins['type'] == \"CDS\"]\n",
        "print(prokka_proteins.shape)\n",
        "unnamed_cols = [col for col in prokka_proteins.columns if col.startswith('Unnamed')]\n",
        "prokka_proteins.drop(columns=unnamed_cols, inplace=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qFfiZZgMJc-0",
        "outputId": "902da785-2953-43c2-b31d-3111597241ff"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(75516, 21)\n",
            "(75516, 21)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the 'start' and 'end' columns to integers if they are not already\n",
        "prokka_proteins['start'] = prokka_proteins['start'].astype(str)\n",
        "prokka_proteins['end'] = prokka_proteins['end'].astype(str)\n",
        "\n",
        "# Filter the DataFrame based on the conditions\n",
        "filtered_prokka_proteins = prokka_proteins[\n",
        "    (prokka_proteins['seq_id'] == \"NODE_16543_length_5404_cov_2.090484_bin.67\") &\n",
        "    (prokka_proteins['start'] == \"22\") &\n",
        "    (prokka_proteins['end'] == \"174\")\n",
        "]\n",
        "\n",
        "# Display the result\n",
        "print(filtered_prokka_proteins)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UzQQiY-14rCG",
        "outputId": "3b43599b-4b86-4a20-a933-8a10acb9e777"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                           seq_id           source type start  \\\n",
            "0      NODE_16543_length_5404_cov_2.090484_bin.67  Prodigal:002006  CDS    22   \n",
            "28071  NODE_16543_length_5404_cov_2.090484_bin.67  Prodigal:002006  CDS    22   \n",
            "\n",
            "       end score strand  phase  \\\n",
            "0      174     .      +      0   \n",
            "28071  174     .      +      0   \n",
            "\n",
            "                                              attributes              ID Name  \\\n",
            "0      ID=GLCIONJA_00001;inference=ab initio predicti...  GLCIONJA_00001  NaN   \n",
            "28071  ID=GLCIONJA_28252;inference=ab initio predicti...  GLCIONJA_28252  NaN   \n",
            "\n",
            "      db_xref eC_number gene                             inference  \\\n",
            "0         NaN       NaN  NaN  ab initio prediction:Prodigal:002006   \n",
            "28071     NaN       NaN  NaN  ab initio prediction:Prodigal:002006   \n",
            "\n",
            "            locus_tag note               product coverage interval bin status  \n",
            "0      GLCIONJA_00001  NaN  hypothetical protein           1.9-2.1     binned  \n",
            "28071  GLCIONJA_28252  NaN  hypothetical protein           1.9-2.1     binned  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prokka_proteins_no_duplicates = prokka_proteins.drop_duplicates(subset=['seq_id', 'start', 'end'], keep='first')\n"
      ],
      "metadata": {
        "id": "EGIdEkam6HJc"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load and prep bath files\n",
        "bath_protein = pd.read_excel(bath_dedup_annotations)\n",
        "unnamed_cols = [col for col in bath_protein.columns if col.startswith('Unnamed')]\n",
        "bath_protein.drop(columns=unnamed_cols, inplace=True)\n",
        "bath_protein['start'] = bath_protein['ali from']\n",
        "bath_protein['end'] = bath_protein['ali to']\n",
        "bath_protein['start'].fillna(bath_protein['ali from flip'],inplace=True)\n",
        "bath_protein['end'].fillna(bath_protein['ali to flip'],inplace=True)\n",
        "bath_protein['ID'] = bath_protein['query name']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vEFkVeEqLUBY",
        "outputId": "c0002f54-fd06-4eef-e50f-bba9fb1889dc"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-12-94347e72fdd2>:7: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  bath_protein['start'].fillna(bath_protein['ali from flip'],inplace=True)\n",
            "<ipython-input-12-94347e72fdd2>:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  bath_protein['end'].fillna(bath_protein['ali to flip'],inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Remove IS elements from both Prokka and BATH data"
      ],
      "metadata": {
        "id": "67-rhNR5b0eH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bath_protein = bath_protein[~bath_protein['query name'].str.startswith(\"IS\")]"
      ],
      "metadata": {
        "id": "s5evhxblb4p_"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prokka_proteins = prokka_proteins[~prokka_proteins['inference'].str.contains(\"ISfinder\")]"
      ],
      "metadata": {
        "id": "jJ4U1yjFb4w1"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bath_protein.shape,prokka_proteins.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pf20rntRe7UW",
        "outputId": "96e0fd88-6c35-4c25-aa16-08536a4fe515"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(29015, 27) (75416, 20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Save new bath and prokka protein files"
      ],
      "metadata": {
        "id": "uNWfd9o6pgzl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "bath_protein.to_excel(bath_dedup_annotations_save,index=False)\n",
        "prokka_proteins.to_excel(prokka_annotations_save,index=False)"
      ],
      "metadata": {
        "id": "04vYiyKWplIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Find the gaps filled by BATH vs Prokka per contig"
      ],
      "metadata": {
        "id": "x2Um3f-1fJa4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Separate BATH data into positive and negative strands\n",
        "pos_strand = bath_protein[bath_protein['strand'] == \"+\"].copy()\n",
        "neg_strand = bath_protein[bath_protein['strand'] == \"-\"].copy()\n",
        "\n",
        "# Handle columns for positive strand\n",
        "pos_strand.loc[:, 'start'] = pos_strand['ali from'].astype(int)\n",
        "pos_strand.loc[:, 'end'] = pos_strand['ali to'].astype(int)\n",
        "\n",
        "# Handle columns for negative strand\n",
        "neg_strand.loc[:, 'start'] = neg_strand['ali from flip'].astype(int)\n",
        "neg_strand.loc[:, 'end'] = neg_strand['ali to flip'].astype(int)\n",
        "\n",
        "# Filter Prokka proteins into positive and negative strands\n",
        "prokka_proteins_plus = prokka_proteins[prokka_proteins['strand'] == '+'].copy()\n",
        "prokka_proteins_minus = prokka_proteins[prokka_proteins['strand'] == '-'].copy()\n",
        "\n",
        "# Sort and save positive strand Prokka proteins\n",
        "prokka_proteins_plus.sort_values('start', inplace=True)\n",
        "prokka_proteins_plus.to_csv('prokka_proteins_plus.bed', sep='\\t', index=False, header=False, columns=['seq_id', 'start', 'end', 'strand', 'ID', 'inference'])\n",
        "\n",
        "# Sort and save negative strand Prokka proteins\n",
        "prokka_proteins_minus.sort_values('start', inplace=True)\n",
        "prokka_proteins_minus.to_csv('prokka_proteins_minus.bed', sep='\\t', index=False, header=False, columns=['seq_id', 'start', 'end', 'strand', 'ID', 'inference'])\n",
        "\n",
        "# Sort and save BATH deduplicated positive strand\n",
        "bath_deduplicated_plus = pos_strand[['target name', 'start', 'end', 'strand', 'ID', 'shifts','E-value']].copy()\n",
        "bath_deduplicated_plus.sort_values('start', inplace=True)\n",
        "bath_deduplicated_plus.to_csv('bath_deduplicated_plus.bed', sep='\\t', index=False, header=False)\n",
        "\n",
        "# Sort and save BATH deduplicated negative strand\n",
        "bath_deduplicated_minus = neg_strand[['target name', 'start', 'end', 'strand', 'ID', 'shifts','E-value']].copy()\n",
        "bath_deduplicated_minus.sort_values('start', inplace=True)\n",
        "bath_deduplicated_minus.to_csv('bath_deduplicated_minus.bed', sep='\\t', index=False, header=False)\n"
      ],
      "metadata": {
        "id": "WgQ2JY2zc1KI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(bath_deduplicated_plus.shape,bath_deduplicated_minus.shape)\n",
        "print(prokka_proteins_plus.shape,prokka_proteins_minus.shape)"
      ],
      "metadata": {
        "id": "VstEIK6Mff5-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Functions\n",
        "\n",
        "\n",
        "\n",
        "1.   generate_bed_files:\n",
        "This function creates BED files by iterating through unique contig identifiers in the DataFrame, filtering and sorting data by start positions, and writing the data to BED files.\n",
        "\n",
        "2.   run_bedmap_operations:\n",
        "This function uses bedmap and bedops tools to find overlapping and unique regions between BATH and Prokka BED files. It generates three output files: one for overlaps, one for regions unique to BATH, and one for regions unique to Prokka, saving them in the specified output directory\n",
        "\n",
        "3.   find_and_process_bed_pairs:\n",
        "This function finds and pairs BED files from BATH and Prokka directories based on sequence identifiers. It then calls run_bedmap_operations to process each pair, identifying overlaps and unique regions, and saves the results in the output directory.\n",
        "\n",
        "4.   combine_files_by_prefix:\n",
        "This function organizes and combines BED files in a source directory by their prefixes. It groups files with the same prefix, concatenates their contents, and writes the combined data into new BED files in the output directory. Each new file is named with the prefix followed by \"combined.bed\"."
      ],
      "metadata": {
        "id": "KDjfnh2fuHEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Functions\n",
        "\n",
        "def generate_bed_files(df, directory, contig_column, file_prefix='Prokka_annotation'):\n",
        "    # Create directory if it doesn't exist\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "    total_lines_processed = 0  # Initialize a counter for total lines processed\n",
        "\n",
        "    # Loop over unique sequence identifiers (contig_column)\n",
        "    for seq_id in df[contig_column].unique():\n",
        "        df_seq = df[df[contig_column] == seq_id]\n",
        "        df_seq = df_seq.sort_values(by=['start'])\n",
        "        filename = f'{directory}/{file_prefix}_{seq_id}.bed'\n",
        "\n",
        "        # Initialize a counter for this seq_id\n",
        "        lines_processed = 0\n",
        "\n",
        "        # Write BED file for this seq_id\n",
        "        with open(filename, 'w') as file:\n",
        "            for _, row in df_seq.iterrows():\n",
        "                if file_prefix == \"Prokka_annotation\":\n",
        "                    bed_line = f\"{row[contig_column]}\\t{row['start']}\\t{row['end']}\\t{row['ID']}\\t{row['strand']}\\t{row['inference']}\\n\"\n",
        "                else:\n",
        "                    bed_line = f\"{row[contig_column]}\\t{row['start']}\\t{row['end']}\\t{row['ID']}\\t{row['strand']}\\t{row['shifts']}\\t{row['E-value']}\\n\"\n",
        "                file.write(bed_line)\n",
        "                lines_processed += 1  # Increment the counter for this seq_id\n",
        "\n",
        "        # Update the total lines processed across all files\n",
        "        total_lines_processed += lines_processed\n",
        "\n",
        "        # Print the number of lines processed for this specific seq_id\n",
        "        print(f\"Processed {lines_processed} lines for {seq_id}\")\n",
        "\n",
        "    # Print the total number of lines processed\n",
        "    print(f\"Total lines processed: {total_lines_processed}\")\n",
        "    print(f\"BED files have been successfully generated in {directory}\")\n",
        "\n",
        "def run_bedmap_operations(bath_bed_path, prokka_bed_path, output_base_dir):\n",
        "    seq_id = bath_bed_path.stem.split('annotation_')[-1]\n",
        "    print(\"Processing this:\",seq_id)\n",
        "    overlap_output = output_base_dir / f'overlap_prokka_bath_{seq_id}.bed'\n",
        "    unique_to_bath_output = output_base_dir / f'unique_to_bath_{seq_id}.bed'\n",
        "    unique_to_prokka_output = output_base_dir / f'unique_to_prokka_{seq_id}.bed'\n",
        "\n",
        "    output_base_dir.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    # Command for bedmap to find overlaps\n",
        "    command = [\n",
        "        \"bedmap\", \"--echo\", \"--echo-map\", \"--delim\", \";\",\n",
        "        \"--fraction-ref\", \"0.15\",\n",
        "        str(bath_bed_path), str(prokka_bed_path)\n",
        "    ]\n",
        "\n",
        "    # Run the command and capture output\n",
        "    result = subprocess.run(command, capture_output=True, text=True, check=True)\n",
        "\n",
        "    # Filter the output and write to overlap_output\n",
        "    with open(overlap_output, 'w') as f_output:\n",
        "        for line in result.stdout.splitlines():\n",
        "            parts = line.split(';')\n",
        "            # Check if there is more than one part and the second part is not empty\n",
        "            if len(parts) > 1 and parts[1].strip():\n",
        "                f_output.write(line + '\\n')\n",
        "\n",
        "    # Run bedmap for unique to BATH\n",
        "    subprocess.run([\"bedops\", \"--not-element-of\", \"1\", str(bath_bed_path), str(prokka_bed_path)], stdout=open(unique_to_bath_output, 'w'), check=True)\n",
        "\n",
        "    # Run bedmap for unique to Prokka\n",
        "    subprocess.run([\"bedops\", \"--not-element-of\", \"1\", str(prokka_bed_path), str(bath_bed_path)], stdout=open(unique_to_prokka_output, 'w'), check=True)\n",
        "\n",
        "def find_and_process_bed_pairs(bath_dir, prokka_dir, output_dir):\n",
        "    bath_dir_path = Path(bath_dir)\n",
        "    prokka_dir_path = Path(prokka_dir)\n",
        "    output_base_dir = Path(output_dir)\n",
        "\n",
        "    # Extract the sequence identifier by removing the prefix and taking the relevant part of the filename.\n",
        "    bath_bed_files = {f.stem.replace('BATH_annotation_', ''): f for f in bath_dir_path.glob('BATH_annotation_*.bed')}\n",
        "    prokka_bed_files = {f.stem.replace('Prokka_annotation_', ''): f for f in prokka_dir_path.glob('Prokka_annotation_*.bed')}\n",
        "\n",
        "    for seq_id, bath_bed_path in bath_bed_files.items():\n",
        "        print(\"Processing sequence ID:\", seq_id)\n",
        "        prokka_bed_path = prokka_bed_files.get(seq_id)\n",
        "        if prokka_bed_path:\n",
        "            print(\"Found matching files: \", prokka_bed_path, \" and \", bath_bed_path)\n",
        "            run_bedmap_operations(bath_bed_path, prokka_bed_path, output_base_dir)\n",
        "        else:\n",
        "            print(f\"No matching Prokka file found for BATH seq_id: {seq_id}\")\n",
        "\n",
        "def combine_files_by_prefix(source_directory, output_directory, prefixes):\n",
        "    source_directory = Path(source_directory)\n",
        "    output_directory = Path(output_directory)\n",
        "    output_directory.mkdir(parents=True, exist_ok=True)\n",
        "    file_groups = {prefix: [] for prefix in prefixes}\n",
        "    for file_path in source_directory.glob('*.bed'):\n",
        "        for prefix in prefixes:\n",
        "            if file_path.name.startswith(prefix):\n",
        "                file_groups[prefix].append(file_path)\n",
        "    for prefix, files in file_groups.items():\n",
        "        combined_file_path = output_directory / f\"{prefix}combined.bed\"\n",
        "        with combined_file_path.open('w') as combined_file:\n",
        "            for file_path in files:\n",
        "                with file_path.open() as input_file:\n",
        "                    combined_file.write(input_file.read())\n",
        "                combined_file.write('\\n')"
      ],
      "metadata": {
        "id": "sKpfkWmmqSr_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "BASE_DIR = '/content/BED'\n",
        "PROKKA_PLUS_DIR = f'{BASE_DIR}/Prokka_Annotation_PLUS_BED'\n",
        "PROKKA_MINUS_DIR = f'{BASE_DIR}/Prokka_Annotation_MINUS_BED'\n",
        "BATH_PLUS_DIR = f'{BASE_DIR}/BATH_Annotation_PLUS_BED'\n",
        "BATH_MINUS_DIR = f'{BASE_DIR}/BATH_Annotation_MINUS_BED'\n",
        "OVERLAPS_DIR = f'{BASE_DIR}/Overlaps and Differences'\n",
        "COMBINED_FILES_DIR = f'{BASE_DIR}/Combined Files'\n",
        "\n",
        "\n",
        "directories = [\n",
        "    PROKKA_PLUS_DIR, PROKKA_MINUS_DIR,\n",
        "    BATH_PLUS_DIR, BATH_MINUS_DIR,\n",
        "    OVERLAPS_DIR, COMBINED_FILES_DIR\n",
        "]\n",
        "for directory in directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "\n",
        "generate_bed_files(prokka_proteins_plus, PROKKA_PLUS_DIR, 'seq_id', file_prefix='Prokka_annotation')\n",
        "generate_bed_files(prokka_proteins_minus, PROKKA_MINUS_DIR, 'seq_id', file_prefix='Prokka_annotation')\n",
        "generate_bed_files(bath_deduplicated_plus, BATH_PLUS_DIR, 'target name', file_prefix='BATH_annotation')\n",
        "generate_bed_files(bath_deduplicated_minus, BATH_MINUS_DIR, 'target name', file_prefix='BATH_annotation')\n",
        "\n",
        "\n",
        "find_and_process_bed_pairs(BATH_PLUS_DIR, PROKKA_PLUS_DIR, OVERLAPS_DIR)\n",
        "\n",
        "prefixes = ['unique_to_prokka_', 'unique_to_bath_', 'overlap_prokka_bath_']\n",
        "combine_files_by_prefix(OVERLAPS_DIR, COMBINED_FILES_DIR, prefixes)"
      ],
      "metadata": {
        "id": "s_FPODJwyzm0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Analysis"
      ],
      "metadata": {
        "id": "j7Zptah4RNCF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Venn diagram of region based overlaps"
      ],
      "metadata": {
        "id": "gH6-YAkM86Ag"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_file_to_dataframe(file_path):\n",
        "    print(f\"Processing file: {file_path}\")  # Display the file being processed\n",
        "\n",
        "    transformed_rows = []\n",
        "    with open(file_path, 'r') as file:\n",
        "        lines = file.readlines()\n",
        "\n",
        "        # Check if the file is empty\n",
        "        if not lines:\n",
        "            raise ValueError(f\"The file '{file_path}' is empty.\")\n",
        "\n",
        "        for line in lines:\n",
        "            fields = line.strip().split(';')\n",
        "\n",
        "            # Ensure that the fields follow the expected format (adjust as needed for your case)\n",
        "            if len(fields) == 3:\n",
        "                new_row1 = [fields[0], fields[1]]  # BATH and Prokka match\n",
        "                new_row2 = [fields[0], fields[2]]  # Add another matching pair if needed\n",
        "                transformed_rows.append(new_row1)\n",
        "                transformed_rows.append(new_row2)\n",
        "            else:\n",
        "                # Handle lines with fewer/more fields if necessary (adjust this based on your file format)\n",
        "                transformed_rows.append(fields)\n",
        "\n",
        "    # Filter out invalid rows (those that don't match expected length) and create DataFrame\n",
        "    valid_rows = [row for row in transformed_rows if len(row) == 2]\n",
        "    df = pd.DataFrame(valid_rows, columns=['BATH', 'Prokka']).replace(\"\\n\", np.nan).dropna()\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "lWeZgw2Sqc9e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/BED/Combined Files/overlap_prokka_bath_combined.bed'\n",
        "overlaps = transform_file_to_dataframe(file_path)"
      ],
      "metadata": {
        "id": "Sm3hcogl89ey"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def determine_label(row):\n",
        "    if row['Prokka'].endswith('ab initio prediction:Prodigal:002006'):\n",
        "        return \"BATH and Prokka Unannotated\"\n",
        "    else:\n",
        "        return \"BATH and Prokka Annotated\"\n",
        "\n",
        "overlaps['label'] = overlaps.apply(determine_label, axis=1)"
      ],
      "metadata": {
        "id": "hZ-8rJkR_9yA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = '/content/BED/Combined Files/unique_to_bath_combined.bed'\n",
        "with open(file_path, 'r') as file:\n",
        "    lines = [line.strip() for line in file.readlines() if line.strip()]\n",
        "unique_to_bath = pd.DataFrame(lines, columns=['BATH']).replace(\"\\n\",np.nan).dropna()\n",
        "unique_to_bath['Prokka'] = np.nan\n",
        "unique_to_bath['label'] = 'BATH'"
      ],
      "metadata": {
        "id": "FghlpHe7_tMk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path3 = '/content/BED/Combined Files/unique_to_prokka_combined.bed'\n",
        "with open(file_path3, 'r') as file:\n",
        "    lines = [line.strip() for line in file.readlines() if line.strip()]\n",
        "unique_to_prokka = pd.DataFrame(lines, columns=['Prokka']).replace(\"\\n\",np.nan).dropna()\n",
        "unique_to_prokka['BATH'] = np.nan\n",
        "\n",
        "def determine_label2(row):\n",
        "    if row['Prokka'].endswith('ab initio prediction:Prodigal:002006'):\n",
        "        return \"Prokka Unannotated\"\n",
        "    else:\n",
        "        return \"Prokka Annotated\"\n",
        "\n",
        "unique_to_prokka['label'] = unique_to_prokka.apply(determine_label2, axis=1)"
      ],
      "metadata": {
        "id": "izo5DWE4E-Zv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "venn_data = pd.concat([unique_to_prokka,unique_to_bath,overlaps])"
      ],
      "metadata": {
        "id": "AsZvSBJ5_tPv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "venn_data['label'].value_counts()"
      ],
      "metadata": {
        "id": "6AqVGwhPIexG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Remove alignments identified by Prokka and not BATH because they are closer to the boundary"
      ],
      "metadata": {
        "id": "H1OjjB426Crj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Take data from prokka annotated and now go back to prededuplicated data to find alignments to this and then capture the lowest E-value, if this E-value is higher than the threshold then throw away this prokka data\n",
        "only_prokka_venn = venn_data[venn_data['label'] == \"Prokka Annotated\"].copy()"
      ],
      "metadata": {
        "id": "lvmCb1_e6NLt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "header = [\n",
        "    \"target name\", \"accession\", \"query name\", \"accession\", \"hmm len\",\n",
        "    \"hmm from\", \"hmm to\", \"seq len\", \"ali from\", \"ali to\",\n",
        "    \"env from\", \"env to\", \"E-value\", \"score\", \"bias\",\n",
        "    \"shifts\", \"stops\", \"pipe\", \"description of target\"\n",
        "]\n",
        "\n",
        "dataframes = []\n",
        "for i in list_of_bathsearch_outputs:\n",
        "    df = pd.read_csv(i, skipfooter=9, skiprows=2, header=None, sep=\"\\s+\", engine='python')\n",
        "    dataframes.append(df)\n",
        "    #print(df)\n",
        "\n",
        "combined_df = pd.concat(dataframes, ignore_index=True)\n",
        "combined_df.columns = header\n",
        "combined_df.to_csv(bathsearch_output_file, index=False)\n",
        "\n",
        "combined_df"
      ],
      "metadata": {
        "id": "MOyugOjo9-3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the 'Prokka' column by '\\t' and expand into new columns\n",
        "expanded_cols = only_prokka_venn['Prokka'].str.split('\\t', expand=True)\n",
        "expanded_cols.columns = ['seq_id', 'start', 'end', 'ID', 'strand', 'inference']\n",
        "only_prokka_venn = pd.concat([expanded_cols, only_prokka_venn], axis=1)"
      ],
      "metadata": {
        "id": "zYm9jG98Ib5K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(only_prokka_venn.columns)\n",
        "print(only_prokka_venn.columns.duplicated())"
      ],
      "metadata": {
        "id": "TfWuLqNR9KSe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "only_prokka_venn"
      ],
      "metadata": {
        "id": "Scdpwqa0wW-Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "only_prokka_venn_pos = only_prokka_venn[only_prokka_venn['strand'] == \"+\"]\n",
        "only_prokka_venn_neg = only_prokka_venn[only_prokka_venn['strand'] == \"-\"]\n",
        "\n",
        "combined_df['strand'] = np.where(combined_df['ali from'] - combined_df['ali to'] < 0, \"-\", \"+\")\n",
        "combined_df['start'] = combined_df['ali from']\n",
        "combined_df['end'] = combined_df['ali to']\n",
        "combined_df['ID'] = combined_df['query name']\n",
        "combined_df_pos = combined_df[combined_df['strand']==\"+\"]\n",
        "combined_df_neg = combined_df[combined_df['strand']==\"-\"]\n"
      ],
      "metadata": {
        "id": "68K4Am-RK11Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define directories for separate analysis\n",
        "SEPARATE_ANALYSIS_DIR = f'{BASE_DIR}/Separate Analysis'\n",
        "SEPARATE_PROKKA_PLUS_DIR = f'{SEPARATE_ANALYSIS_DIR}/Prokka_Annotation_PLUS_BED'\n",
        "SEPARATE_PROKKA_MINUS_DIR = f'{SEPARATE_ANALYSIS_DIR}/Prokka_Annotation_MINUS_BED'\n",
        "SEPARATE_BATH_PLUS_DIR = f'{SEPARATE_ANALYSIS_DIR}/BATH_Annotation_PLUS_BED'\n",
        "SEPARATE_BATH_MINUS_DIR = f'{SEPARATE_ANALYSIS_DIR}/BATH_Annotation_MINUS_BED'\n",
        "SEPARATE_OVERLAPS_DIR = f'{SEPARATE_ANALYSIS_DIR}/Overlaps and Differences'\n",
        "SEPARATE_Combined_DIR = f'{SEPARATE_ANALYSIS_DIR}/Combined Files'\n",
        "SEPARATE_OVERLAPS_DIR = f'{SEPARATE_ANALYSIS_DIR}/Overlaps and Differences'\n",
        "SEPARATE_Combined_DIR_OPP = f'{SEPARATE_ANALYSIS_DIR}/Combined Files Opposite Strands'\n",
        "SEPARATE_OVERLAPS_DIR_OPP = f'{SEPARATE_ANALYSIS_DIR}/Overlaps and Differences Opposite Strands'"
      ],
      "metadata": {
        "id": "Pe-vFDIxNw1D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "separate_directories = [\n",
        "    SEPARATE_ANALYSIS_DIR, SEPARATE_PROKKA_PLUS_DIR, SEPARATE_PROKKA_MINUS_DIR,\n",
        "    SEPARATE_BATH_PLUS_DIR, SEPARATE_BATH_MINUS_DIR, SEPARATE_OVERLAPS_DIR, SEPARATE_Combined_DIR,\n",
        "    SEPARATE_Combined_DIR_OPP, SEPARATE_OVERLAPS_DIR_OPP\n",
        "]\n",
        "\n",
        "for directory in separate_directories:\n",
        "    os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Generate BED files for Prokka and BATH annotations for both positive and negative strands in the new directories\n",
        "generate_bed_files(only_prokka_venn_pos, SEPARATE_PROKKA_PLUS_DIR, 'seq_id', file_prefix='Prokka_annotation')\n",
        "generate_bed_files(only_prokka_venn_neg, SEPARATE_PROKKA_MINUS_DIR, 'seq_id', file_prefix='Prokka_annotation')\n",
        "generate_bed_files(combined_df_pos, SEPARATE_BATH_PLUS_DIR, 'target name', file_prefix='BATH_annotation')\n",
        "generate_bed_files(combined_df_neg, SEPARATE_BATH_MINUS_DIR, 'target name', file_prefix='BATH_annotation')\n",
        "\n",
        "# Process BED pairs to find overlaps and unique regions between BATH and Prokka in the new directories\n",
        "find_and_process_bed_pairs(SEPARATE_BATH_PLUS_DIR, SEPARATE_PROKKA_PLUS_DIR, SEPARATE_OVERLAPS_DIR)\n",
        "find_and_process_bed_pairs(SEPARATE_BATH_MINUS_DIR, SEPARATE_PROKKA_MINUS_DIR, SEPARATE_OVERLAPS_DIR)\n",
        "\n",
        "#Process opposite strand files\n",
        "find_and_process_bed_pairs(SEPARATE_BATH_MINUS_DIR, SEPARATE_PROKKA_PLUS_DIR, SEPARATE_OVERLAPS_DIR_OPP)\n",
        "find_and_process_bed_pairs(SEPARATE_BATH_PLUS_DIR, SEPARATE_PROKKA_MINUS_DIR, SEPARATE_OVERLAPS_DIR_OPP)\n",
        "\n",
        "# Combine resulting BED files by specified prefixes in the new directories\n",
        "prefixes = ['unique_to_prokka_', 'unique_to_bath_', 'overlap_prokka_bath_']\n",
        "combine_files_by_prefix(SEPARATE_OVERLAPS_DIR, SEPARATE_Combined_DIR, prefixes)\n",
        "combine_files_by_prefix(SEPARATE_OVERLAPS_DIR_OPP, SEPARATE_Combined_DIR_OPP, prefixes)"
      ],
      "metadata": {
        "id": "NSX07VOIK14v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Annotations with hits on -ve strands"
      ],
      "metadata": {
        "id": "9TZQm61_IW7Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_prokka_opposite_strand = '/content/BED/Separate Analysis/Combined Files Opposite Strands/overlap_prokka_bath_combined.bed'\n",
        "overlaps_prokka_opposite_strand = transform_file_to_dataframe(file_path_prokka_opposite_strand)\n",
        "overlaps_prokka_opposite_strand"
      ],
      "metadata": {
        "id": "fZ5CjtgmIUJP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overlaps_prokka_opposite_strand.groupby(['Prokka']).agg(\";\".join)"
      ],
      "metadata": {
        "id": "t86f3zsNJAPt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_drop = overlaps_prokka_opposite_strand.groupby(['Prokka']).agg(\";\".join).index.unique()"
      ],
      "metadata": {
        "id": "IRPQXsuHV2Fl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "venn_data_filtered = venn_data[~venn_data['Prokka'].isin(columns_to_drop)]\n",
        "print(venn_data_filtered)"
      ],
      "metadata": {
        "id": "E2sAWTiCUzRW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Annotations with hits having E -values closer to threshold\n",
        "\n"
      ],
      "metadata": {
        "id": "LlV5dbu-d0g2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "file_path_prokka_only_same_strand = '/content/BED/Separate Analysis/Combined Files/overlap_prokka_bath_combined.bed'\n",
        "overlaps_prokka_only_same_strand = transform_file_to_dataframe(file_path_prokka_only_same_strand)\n",
        "overlaps_prokka_only_same_strand"
      ],
      "metadata": {
        "id": "rSWXpImrIFlN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def filter_venn_data(overlaps_prokka_only_same_strand):\n",
        "    filtered_prokka_values = []\n",
        "    for index, row in overlaps_prokka_only_same_strand.iterrows():\n",
        "        bath_value = float(row['BATH'].split(\"\\t\")[-1])\n",
        "        if bath_value > 10**-6:\n",
        "            filtered_prokka_values.append(row['Prokka'])\n",
        "\n",
        "    filtered_prokka_values = list(set(filtered_prokka_values))\n",
        "\n",
        "    return filtered_prokka_values\n"
      ],
      "metadata": {
        "id": "UVgwxVF7evuu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "filtered_prokka_values = filter_venn_data(overlaps_prokka_only_same_strand)"
      ],
      "metadata": {
        "id": "8iZF291ve8Iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "venn_data_filtered2 = venn_data_filtered[~venn_data_filtered['Prokka'].isin(filtered_prokka_values)]"
      ],
      "metadata": {
        "id": "6hb26yCNv2X7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Measuring Overlaps"
      ],
      "metadata": {
        "id": "QOEDccULXOR-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prokka_unannotated = venn_data_filtered2[venn_data_filtered2['label'] == \"Prokka Unannotated\"].shape[0]\n",
        "only_bath = venn_data_filtered2[venn_data_filtered2['label'] == \"BATH\"].shape[0]\n",
        "intersection_3 = venn_data_filtered2[venn_data_filtered2['label'] == \"BATH and Prokka Annotated\"].shape[0]\n",
        "bath_prokka_unannotated = venn_data_filtered2[venn_data_filtered2['label'] == \"BATH and Prokka Unannotated\"].shape[0] + venn_data_filtered2[venn_data_filtered2['label'] == \"BATH and Prokka Annotated\"].shape[0]\n",
        "prokka_unannot_annot = venn_data_filtered2[venn_data_filtered2['label'] == \"Prokka Annotated\"].shape[0]\n"
      ],
      "metadata": {
        "id": "EsIzLrkeIFk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from matplotlib_venn import venn3\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use('seaborn')\n",
        "venn_counts = {\n",
        "    '100': only_bath,  # Only BATH\n",
        "    '010': 0,  # Only Prokka Annotated (Nothing since everything is prodigal and prokka annotated)\n",
        "    '001': prokka_unannotated,  # Only Prokka Unannotated\n",
        "    '110': 0,  # Intersection BATH and Prokka Annotated (Nothing since everything is prodigal and prokka annotated)\n",
        "    '101': venn_data_filtered2[venn_data_filtered2['label'] == \"BATH and Prokka Unannotated\"].shape[0],  # Intersection BATH and Prokka Unannotated\n",
        "    '011': prokka_unannot_annot,  # Intersection of Prokka Annotated and Prokka Unannotated (not given)\n",
        "    '111': intersection_3   # Intersection of all three (not applicable)\n",
        "}\n",
        "\n",
        "\n",
        "set_colors = (\"#de3028\", \"#1B263B\", \"#FFD700\")\n",
        "venn3(subsets=venn_counts, set_labels=('BATH Annotations', 'Prokka Annotations', 'ORFs (Prodigal)'), set_colors=set_colors)\n",
        "\n",
        "\n",
        "plt.title(\"Overlaping annotations BATH and Prokka based DNA position\")\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lufM0t3bIFod"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "venn_data_filtered2.to_excel(bath_prokka_alignment,index=False)"
      ],
      "metadata": {
        "id": "F78xwpca3oYr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "R-KYZ4JVNufj"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}